{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.28.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' The game \\'s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \\n'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "datasets[\"test\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Made on a budget of around ₹ 100 — 120 million , the film 's principal photography commenced in November 2005 and lasted until April 2006 . Most of the film was shot in and around Hyderabad and Chennai , except for a song which was shot at the province of Phuket in Thailand and the city of Bangkok . Shyam K. Naidu was the film 's cinematographer , and it was edited by Marthand K. Venkatesh . The soundtrack and background score were composed by Mani Sharma . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is a sign of Edward 's high regard for Lancaster that he would bestow such extensive privileges on him . The two men were second cousins through their great @-@ grandfather Henry III and practically coeval ( Edward was born in 1312 ) , so it is natural to assume that a strong sense of camaraderie existed between them . Another factor that might have influenced the king 's decision was the fact that Henry had no male heir , so the grant was made for the Earl 's lifetime only , and not intended to be hereditary . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>After many delays , the General and his party arrive in Honda , where the Governor , Posada Gutiérrez , has arranged for three days of fiestas . On his last night in Honda , the General returns late to camp and finds one of his old friends , Miranda Lyndsay , waiting for him . The General recalls that fifteen years ago , she had learned of a plot against his life and had saved him . The following morning , the General begins the voyage down the Magdalena River . Both his physical debilitation and pride are evident as he negotiates the slope to the dock : he is in need of a sedan chair but refuses to use it . The group stays a night in Puerto Real , where the General claims he sees a woman singing during the night . His aides @-@ de @-@ camp and the watchman conduct a search , but they fail to uncover any sign of a woman having been in the vicinity . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Many Egyptologists and anthropologists have suggested theories about how the gods developed in these early times . Gustave Jéquier , for instance , thought the Egyptians first revered primitive fetishes , then deities in animal form , and finally deities in human form , whereas Henri Frankfort argued that the gods must have been envisioned in human form from the beginning . Some of these theories are now regarded as too simplistic , and more current ones , such as Siegfried Morenz ' hypothesis that deities emerged as humans began to distinguish themselves from and personify their environment , are difficult to prove . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Disassembly of the Type 94 Nambu pistol is considered difficult and can lead to damage to the pistol if done carelessly . After clearing the Type 94 , the operator must draw the slide against the magazine follower to hold the bolt to the rear of the pistol . This will allow the crossbolt to be released after the firing pin is depressed . Removal of the crossbolt without depressing the firing pin will damage both the firing pin and the crossbolt . Removing the crossbolt is further complicated as the disassemblers hands are both holding the pistol and depressing the firing pin . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Suresh Krishnamoorthy of The Hindu summarised , \" Magadheera is not for the weak @-@ hearted , those who do not like the sight of blood and neither is it for those who like movies with storylines that are much @-@ closer @-@ to @-@ everyday- reality . But Rajamouli excels in story @-@ telling . The way he has used the flashback as a flip switch , going back and forth and taking the viewer through a 400 @-@ year journey in a jiffy is interesting . The rest is about how well technology , creativity , imagination and innovation are leveraged to present what is an eye @-@ pleasing experience for viewers . \" B. V. S. Prakash of The Times of India wrote , \" Despite a few narrative lapses , the much @-@ hyped semi @-@ periodic epic lives up to expectations . Unlike his previous action @-@ centric films , director Rajamouli dishes a heart @-@ touching love story in a lavish canvas convincingly . Also kudos for the way he has visualised and presented the film . \" He added , \" After not @-@ so @-@ impressive Chirutha , Ram Charan Tej returns as a valiant soldier and breathes life into the larger @-@ than @-@ life role with ease . Similarly , Kajal known for simple lover girl roles transforms into a determined princess and truly impresses . Dev Gil is adequate as the ruthless villain \" , and rated the film 3 out of 5 . \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilgpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd455f56580448198038e0759a26b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7758154fbc054cee82bb40f877bcfe20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e6899f644d40b4bcebd2ff29871445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19abc66e3fd8448290af942d9e262d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-262948adfd561d96_*_of_00004.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-f2fa497fbe27fc6c_*_of_00004.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-4d919120a52d1192_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://openai.com/research/better-language-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = tokenizer.model_max_length\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    # Create a new dictionary by iterating over the keys in the input `examples`\n",
    "    # dictionary and concatenating the lists of tokens for each key.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    \n",
    "    # Calculate the total length of the concatenated list of tokens\n",
    "    # for the first key in the `examples` dictionary.\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    # Drop the small remainder that is smaller than `block_size`.\n",
    "    # You could modify this part to pad the text if the model supports padding\n",
    "    # instead of dropping the remainder.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    \n",
    "    # Split the concatenated lists of tokens into chunks of size `block_size`.\n",
    "    # Create a new dictionary by iterating over the `concatenated_examples`\n",
    "    # dictionary and using list comprehension to create a list of chunks\n",
    "    # for each key in the dictionary.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # Add the \"labels\" key to the `result` dictionary with the same value\n",
    "    # as the \"input_ids\" key.\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-5fa659d1b8d5ed60_*_of_00004.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-502f4991e270aa62_*_of_00004.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-eb096a1839e0dbce_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[983,\n",
       " 290,\n",
       " 5679,\n",
       " 262,\n",
       " 366,\n",
       " 17871,\n",
       " 5321,\n",
       " 366,\n",
       " 837,\n",
       " 257,\n",
       " 23634,\n",
       " 2422,\n",
       " 4326,\n",
       " 7351,\n",
       " 262,\n",
       " 3277,\n",
       " 286,\n",
       " 7096,\n",
       " 544,\n",
       " 1141,\n",
       " 262,\n",
       " 5498,\n",
       " 1898,\n",
       " 6839,\n",
       " 1810,\n",
       " 508,\n",
       " 1620,\n",
       " 3200,\n",
       " 2042,\n",
       " 4560,\n",
       " 290,\n",
       " 389,\n",
       " 46852,\n",
       " 1028,\n",
       " 262,\n",
       " 11773,\n",
       " 4326,\n",
       " 366,\n",
       " 2199,\n",
       " 321,\n",
       " 265,\n",
       " 88,\n",
       " 12552,\n",
       " 366,\n",
       " 764,\n",
       " 220,\n",
       " 198,\n",
       " 383,\n",
       " 983,\n",
       " 2540,\n",
       " 2478,\n",
       " 287,\n",
       " 3050,\n",
       " 837,\n",
       " 6872,\n",
       " 625,\n",
       " 257,\n",
       " 1588,\n",
       " 6903,\n",
       " 286,\n",
       " 262,\n",
       " 670,\n",
       " 1760,\n",
       " 319,\n",
       " 569,\n",
       " 18354,\n",
       " 7496,\n",
       " 17740,\n",
       " 2873,\n",
       " 764,\n",
       " 2893,\n",
       " 340,\n",
       " 17383,\n",
       " 262,\n",
       " 3210,\n",
       " 3033,\n",
       " 286,\n",
       " 262,\n",
       " 2168,\n",
       " 837,\n",
       " 340,\n",
       " 635,\n",
       " 25289,\n",
       " 3294,\n",
       " 16895,\n",
       " 837,\n",
       " 884,\n",
       " 355,\n",
       " 1642,\n",
       " 262,\n",
       " 983,\n",
       " 517,\n",
       " 43486,\n",
       " 329,\n",
       " 2168,\n",
       " 29661,\n",
       " 764,\n",
       " 15684,\n",
       " 11915,\n",
       " 371,\n",
       " 4548,\n",
       " 64,\n",
       " 8835,\n",
       " 73,\n",
       " 280,\n",
       " 290,\n",
       " 26777,\n",
       " 7286,\n",
       " 13704,\n",
       " 13231,\n",
       " 43354,\n",
       " 1111,\n",
       " 4504,\n",
       " 422,\n",
       " 2180,\n",
       " 12784,\n",
       " 837,\n",
       " 1863,\n",
       " 351,\n",
       " 569,\n",
       " 18354,\n",
       " 7496,\n",
       " 17740,\n",
       " 2873,\n",
       " 3437,\n",
       " 33687,\n",
       " 5303,\n",
       " 18024]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])\n",
    "#lm_datasets[\"train\"][1][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec8d79575e846739f620324e1866e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a818eae3f414174802d712d760af05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%nvidia-smi` not found.\n"
     ]
    }
   ],
   "source": [
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-wikitext2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='487' max='7002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 487/7002 00:38 < 08:34, 12.65 it/s, Epoch 0.21/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7002, training_loss=3.6940719264808703, metrics={'train_runtime': 608.2752, 'train_samples_per_second': 92.06, 'train_steps_per_second': 11.511, 'total_flos': 1829011929956352.0, 'train_loss': 3.6940719264808703, 'epoch': 3.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.172572558779414"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(eval_results['eval_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/mnt/CEPH/transformers/distillgpt2/tokenizer_config.json',\n",
       " '/mnt/CEPH/transformers/distillgpt2/special_tokens_map.json',\n",
       " '/mnt/CEPH/transformers/distillgpt2/vocab.json',\n",
       " '/mnt/CEPH/transformers/distillgpt2/merges.txt',\n",
       " '/mnt/CEPH/transformers/distillgpt2/added_tokens.json',\n",
       " '/mnt/CEPH/transformers/distillgpt2/tokenizer.json')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"/mnt/CEPH/transformers/distillgpt2/\")\n",
    "tokenizer.save_pretrained(\"/mnt/CEPH/transformers/distillgpt2/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"path/to/your/local/model/directory\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"path/to/your/local/model/directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"/mnt/CEPH/transformers/distillgpt2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Somatic hypermutation allows the immune system to attack any species in an attempt to damage them by making it more like a \" virus \" or \" virus being distributed over the body. \\n = = Diagnosis and diagnosis = = \\n When'}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Somatic hypermutation allows the immune system to\"\n",
    "generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5583a1f946e04357fa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 83, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface.from_pipeline(pipe)\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
